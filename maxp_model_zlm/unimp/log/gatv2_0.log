[23:23:09] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory
################ Graph info: ###############
Graph(num_nodes=3655452, num_edges=61832630,
      ndata_schemes={}
      edata_schemes={})
################ Label info: ################
Total labels (including not labeled): 3655452
               Training label number: 835542
             Validation label number: 208875
                   Test label number: 592391
################ Feature info: ###############
Node's feature shape:torch.Size([3655452, 300])
Walk's feature shape:torch.Size([3655452, 128])
Edge's feature shape:torch.Size([3655452, 2])
2021-12-14 23:23:52,984 - INFO: Model = UniCMP(
  (graphsage): ModuleList(
    (0): GraphSageLayer(
      (graph): SAGEConv(
        (feat_drop): Dropout(p=0.0, inplace=False)
        (lstm): LSTM(300, 300, batch_first=True)
        (fc_self): Linear(in_features=300, out_features=512, bias=False)
        (fc_neigh): Linear(in_features=300, out_features=512, bias=False)
      )
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): GraphSageLayer(
      (graph): SAGEConv(
        (feat_drop): Dropout(p=0.0, inplace=False)
        (lstm): LSTM(512, 512, batch_first=True)
        (fc_self): Linear(in_features=512, out_features=512, bias=False)
        (fc_neigh): Linear(in_features=512, out_features=512, bias=False)
      )
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (graphattn): ModuleList(
    (0): GraphAttnLayer(
      (graph): GATv2Conv(
        (fc_src): Linear(in_features=300, out_features=512, bias=True)
        (fc_dst): Linear(in_features=300, out_features=512, bias=True)
        (feat_drop): Dropout(p=0.0, inplace=False)
        (attn_drop): Dropout(p=0, inplace=False)
        (leaky_relu): LeakyReLU(negative_slope=0.2)
      )
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): GraphAttnLayer(
      (graph): GATv2Conv(
        (fc_src): Linear(in_features=512, out_features=512, bias=True)
        (fc_dst): Linear(in_features=512, out_features=512, bias=True)
        (feat_drop): Dropout(p=0.0, inplace=False)
        (attn_drop): Dropout(p=0, inplace=False)
        (leaky_relu): LeakyReLU(negative_slope=0.2)
      )
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (graphskip): ModuleList(
    (0): ShortCutLayer(
      (graph): Linear(in_features=300, out_features=512, bias=True)
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): ShortCutLayer(
      (graph): Linear(in_features=300, out_features=512, bias=True)
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): ShortCutLayer(
      (graph): Linear(in_features=512, out_features=512, bias=True)
      (norm): LayerNorm()
      (dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (attn_layers): ModuleList(
    (0): MultiHeadedAttention(
      (query): Linear(in_features=512, out_features=512, bias=True)
      (key): Linear(in_features=512, out_features=512, bias=True)
      (value): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (softmax): Softmax(dim=-1)
    )
    (1): MultiHeadedAttention(
      (query): Linear(in_features=512, out_features=512, bias=True)
      (key): Linear(in_features=512, out_features=512, bias=True)
      (value): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (softmax): Softmax(dim=-1)
    )
  )
  (norm_layers): ModuleList(
    (0): LayerNorm()
    (1): LayerNorm()
  )
  (label_embed): Embedding(24, 300, padding_idx=23)
  (feat_mlp): Sequential(
    (0): Linear(in_features=738, out_features=512, bias=True)
    (1): LayerNorm()
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=512, out_features=300, bias=True)
  )
  (head): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): LayerNorm()
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=512, out_features=23, bias=True)
  )
  (label_dropout): Dropout(p=0.1, inplace=False)
  (feat_dropout): Dropout(p=0.05, inplace=False)
  (dropout): Dropout(p=0.2, inplace=False)
)
2021-12-14 23:23:52,985 - INFO: Model config = {'GNN_MODEL': 'unicmp', 'INPUT_SIZE': 300, 'NUM_LAYERS': 2, 'NUM_HEADS': 8, 'HIDDEN_SIZE': 512, 'NUM_CLASS': 23, 'LABEL_DROP': 0.1, 'FEAT_DORP': 0.05, 'GRAPH_DORP': 0.0, 'ATTN_DROP': 0.0, 'DROP': 0.2, 'USE_SAGE': True, 'USE_CONV': False, 'USE_ATTN': True, 'USE_RESNET': True, 'USE_DESNET': True, 'CHECKPOINT': ''}
2021-12-14 23:23:52,985 - INFO: Dataset config = {'SEED': 2021, 'DATA_PATH': '../dataset', 'DEEPWALK_PATH': 'czw_deep_walk2.npy', 'NEIGHBOR_FEATURES_PATH': 'features_neigh01.npy', 'K_FOLD': 0, 'EPOCHS': 35, 'BATCH_SIZE': 1024, 'BIDIRECTED': True, 'LOG_STEP': 200, 'SAMPLER': 'in', 'FANOUTS': [30, 30], 'IN_FANOUTS': [15, 15], 'OUT_FANOUTS': [15, 15], 'NUM_CLASS': 23, 'REPLACE_LABEL': 0.04, 'MASK_LABEL': 0.16, 'EDGE_DROP': 0.2, 'NUM_WORKERS': 2, 'GRADIENT_ACCUMULATION_STEPS': 1, 'NORM_FEATURE': False, 'OUT_PATH': '../dgl_model/unicmp_gatv2_neigh/unicmp_split0', 'MODEL_PREFIX': 'model', 'TEST_PREFIX': 'model'}
2021-12-14 23:23:52,985 - INFO: Optimizer config = {'LEARNING_RATE': 0.0005, 'COEF_LR': 1.0, 'SCHEDULE': 'warmup_linear', 'WEIGHT_DECAY': 0.0001, 'WARMUP_PROPORTION': 0.1}
2021-12-14 23:23:52,985 - INFO: Criterion config = {'LOSS_TYPE': 'CL', 'NUM_CLASS': 23, 'GAMMA_NEG': 4, 'GAMMA_POS': 1, 'SMOOTHING': 0.1}
2021-12-14 23:23:52,985 - INFO: Training parameters = 7462723
2021-12-14 23:23:52,986 - INFO: Model parameters = 7462723
2021-12-14 23:23:52,986 - INFO: Num steps = 28560
Using backend: pytorch
2021-12-14 23:26:20,260 - INFO:   Epoch: 1/35, Step: 200/816, Lr: 0.000035, Loss: 1.526606, Acc: 0.552734, Time/step: 0.711526
