{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入库\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.dataloading.neighbor import MultiLayerNeighborSampler\n",
    "from dgl.dataloading.pytorch import NodeDataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from yaml import load, Loader\n",
    "from utils import load_dgl_graph, set_seed_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Encoder模型\n",
    "class GraphAttnEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 heads,\n",
    "                 activation,\n",
    "                 feat_drop,\n",
    "                 attn_drop\n",
    "                 ):\n",
    "        super(GraphAttnEncoder, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "        self.feat_dropout = feat_drop\n",
    "        self.attn_dropout = attn_drop\n",
    "        self.activation = activation\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # build multiple layers\n",
    "        self.layers.append(dglnn.GATConv(in_feats=self.in_feats,\n",
    "                                         out_feats=self.hidden_dim,\n",
    "                                         num_heads=self.heads[0],\n",
    "                                         feat_drop=self.feat_dropout,\n",
    "                                         attn_drop=self.attn_dropout,\n",
    "                                         activation=self.activation))\n",
    "\n",
    "        for l in range(1, (self.n_layers)):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.layers.append(dglnn.GATConv(in_feats=self.hidden_dim * self.heads[l - 1],\n",
    "                                             out_feats=self.hidden_dim,\n",
    "                                             num_heads=self.heads[l],\n",
    "                                             feat_drop=self.feat_dropout,\n",
    "                                             attn_drop=self.attn_dropout,\n",
    "                                             activation=self.activation))\n",
    "\n",
    "    def forward(self, blocks, features):\n",
    "        h = features\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            h = self.layers[l](blocks[l], h).flatten(1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "class SimLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimLoss,self).__init__()\n",
    "\n",
    "    def forward(self, out_1, out_2,temperature=0.5):\n",
    "        # 分母 ：X.X.T，再去掉对角线值，分析结果一行，可以看成它与除了这行外的其他行都进行了点积运算（包括out_1和out_2）,\n",
    "        # 而每一行为一个batch的一个取值，即一个输入图像的特征表示，\n",
    "        # 因此，X.X.T，再去掉对角线值表示，每个输入图像的特征与其所有输出特征（包括out_1和out_2）的点积，用点积来衡量相似性\n",
    "        # 加上exp操作，该操作实际计算了分母\n",
    "        B, _ = out_1.shape \n",
    "        # [2*B, D]\n",
    "        out = torch.cat([out_1, out_2], dim=0)\n",
    "        # [2*B, 2*B]\n",
    "        sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
    "        mask = (torch.ones_like(sim_matrix) - torch.eye(2 * B, device=sim_matrix.device)).bool()\n",
    "        # [2*B, 2*B-1]\n",
    "        sim_matrix = sim_matrix.masked_select(mask).view(2 * B, -1)\n",
    "\n",
    "        # 分子： *为对应位置相乘，也是点积\n",
    "        # compute loss\n",
    "        pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "        # [2*B]\n",
    "        pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "        return (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义dataloader\n",
    "# class simDataset(Dataset):\n",
    "#     def __init__(self, n_ids):\n",
    "#         self.n_ids = n_ids\n",
    "#     def __len__(self):\n",
    "#         return len(self.n_ids)\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.n_ids[idx]\n",
    "def graph_resample():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset_cfg, graph_data):\n",
    "    graph, labels, train_nid, val_nid, test_nid, node_feat = graph_data\n",
    "    sampler = MultiLayerNeighborSampler(dataset_cfg['FANOUTS'])  # 50\n",
    "\n",
    "    train_dataloader = NodeDataLoader(graph,\n",
    "                                      train_nid,\n",
    "                                      sampler,\n",
    "                                      batch_size=dataset_cfg['BATCH_SIZE'],\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=dataset_cfg['NUM_WORKERS'])\n",
    "    val_dataloader = NodeDataLoader(graph,\n",
    "                                    val_nid,\n",
    "                                    sampler,\n",
    "                                    batch_size=dataset_cfg['BATCH_SIZE'],\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=False,\n",
    "                                    num_workers=dataset_cfg['NUM_WORKERS'])\n",
    "\n",
    "    return train_dataloader, val_dataloader, node_feat, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subtensor(node_feats, labels, seeds, input_nodes, n_classes, device, training=False):\n",
    "    \"\"\"\n",
    "    Copys features and labels of a set of nodes onto GPU.\n",
    "    \"\"\"\n",
    "    input_feats = node_feats[input_nodes].to(device)\n",
    "\n",
    "    masklabels = labels.clone()\n",
    "    masklabels[seeds] = -1\n",
    "    input_labels = masklabels[input_nodes]\n",
    "\n",
    "    if training:\n",
    "        rd_m = torch.rand(input_labels.shape[0])\n",
    "        rd_y = torch.randint(0, n_classes, size=input_labels.shape)\n",
    "        input_labels[rd_m < 0.15] = -1\n",
    "        input_labels[rd_m > 0.97] = rd_y[rd_m > 0.97]\n",
    "\n",
    "    input_labels[input_labels < 0] = n_classes\n",
    "    input_labels = input_labels.to(device)\n",
    "\n",
    "    batch_labels = labels[seeds].to(device)\n",
    "    return input_feats, input_labels, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_dataloader, node_feat, labels, optimizer, criterion, n_classes,\n",
    "                device, global_step, log_step):\n",
    "    # 获得数据对\n",
    "            input_nodes, output_nodes, blocks = batch_data\n",
    "            input_feats, input_labels, batch_labels = load_subtensor(node_feat,\n",
    "                                                                 labels,\n",
    "                                                                 output_nodes,\n",
    "                                                                 input_nodes,\n",
    "                                                                 dataset_cfg['N_CLASS'],\n",
    "                                                                 device,\n",
    "                                                                 training=False)\n",
    "            blocks = [block.to(device) for block in blocks]\n",
    "\n",
    "            _batch_data = train_dataloader.collator.collate(output_nodes)\n",
    "            _input_nodes, _output_nodes, _blocks = _batch_data\n",
    "            _input_feats, _input_labels, _batch_labels = load_subtensor(node_feat,\n",
    "                                                        labels,\n",
    "                                                        _output_nodes,\n",
    "                                                        _input_nodes,\n",
    "                                                        dataset_cfg['N_CLASS'],\n",
    "                                                        device,\n",
    "                                                        training=False)\n",
    "\n",
    "            _blocks = [_block.to(device) for _block in _blocks]\n",
    "\n",
    "\n",
    "            pre_L = model(blocks, input_feats, input_labels)\n",
    "            pre_R = model(_blocks, _input_feats, _input_labels)\n",
    "\n",
    "            loss=lossLR(pre_L,pre_R)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train stage one\n",
    "def train(model_cfg, dataset_cfg, train_cfg, device, graph_data):\n",
    "\n",
    "    # 获得数据\n",
    "    train_dataloader, val_dataloader, node_feat, labels = get_dataloader(dataset_cfg, graph_data)\n",
    "    \n",
    "    gat_cfg = model_cfg['GAT']\n",
    "\n",
    "    model = GraphAttnEncoder(in_feats=gat_cfg['IN_FEATS'], \n",
    "                            hidden_dim=gat_cfg['HIDDEN_DIM'],\n",
    "                            n_layers=gat_cfg['HIDDEN_DIM'],\n",
    "                            activation=F.relu,\n",
    "                            feat_drop=gat_cfg['GAT_CFG'],\n",
    "                            attn_drop=gat_cfg['ATTN_DROP'])\n",
    "                            \n",
    "    lossLR=SimLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    for epoch in range(1, train_cfg['EPOCH']+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, batch_data in enumerate(train_dataloader):\n",
    "            \n",
    "\n",
    "        print(\"epoch loss:\",total_loss/len(train_dataset)*args.batch_size)\n",
    "\n",
    "        with open(os.path.join(config.save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss/len(train_dataset)*args.batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5==0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "f = open(config_path, 'r', encoding='utf-8')\n",
    "config = f.read()\n",
    "config = load(config, Loader=Loader)\n",
    "dataset_cfg = config['DATASET']\n",
    "model_cfg = config['MODEL']\n",
    "train_cfg = config['TRAIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Graph info: ###############\n",
      "Graph(num_nodes=3655452, num_edges=61832630,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "demo:   (835533,)\n",
      "(208884,)\n",
      "################ Label info: ################\n",
      "Total labels (including not labeled): 3655452\n",
      "               Training label number: 835533\n",
      "             Validation label number: 208884\n",
      "                   Test label number: 592391\n",
      "################ Feature info: ###############\n",
      "Node's feature shape:torch.Size([3655452, 300])\n"
     ]
    }
   ],
   "source": [
    "# 加载图\n",
    "k_fold = 2\n",
    "graph_data = load_dgl_graph(dataset_cfg['DATA_PATH'],\n",
    "                                k_fold,\n",
    "                                norm_feature=dataset_cfg['NORM_FEATURE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train(model_cfg, dataset_cfg, train_cfg,  device, graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical(probs: torch.Size([2, 3]))\n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "probs = torch.FloatTensor([[0.05,0.1,0.85],[0.05,0.05,0.9]])\n",
    " \n",
    "dist = Categorical(probs)\n",
    "print(dist)\n",
    "# Categorical(probs: torch.Size([2, 3]))\n",
    " \n",
    "index = dist.sample()\n",
    "print(index.numpy())\n",
    "# [2 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "index = dist.sample()\n",
    "print(index.numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9b533eabee0874e3229f367e8ff4db86a6869b1260c5f209b8b01bcf670b341"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
